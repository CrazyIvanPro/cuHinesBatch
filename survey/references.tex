\clearpage
\newpage
\chapterimage{head-ref} % Table of contents heading image
\begin{thebibliography}{1}
  \bibitem{Stochastic}
  S. Karlin, and H. M. Taylor. A First Course in Stochastic Processes. Academic Press, 1975.
  \bibitem{chapter2-1}
  M. Gell-Mann and F. E. Low. Quantum Electrodynamics at Small Distances. Phys. Rev. 95, 1300, 1954
  \bibitem{chapter-4}
  Steven Sanche et. al. High Contagiousness and Rapid Spread of Severe Acute Respiratory Syndrome Coronavirus 2, 
  % 1.0 
  \bibitem{foundation}
  Jennings Nicholas R, and Wooldridge Michael J. Foundations of Machine Learning. Foundations of machine learning. MIT Press, 2012.
  % 1.1
  % 1.2
  \bibitem{sanjeev18}
  Sanjeev A., Nadav C., and    Elad H. On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization. In 35th International Conference on Machine Learning, 2018.
  \bibitem{huishuai19}
  Huishuai Z., Da Y., Mingyang Y., Wei C., and Tie-yan L. Convergence Theory of Learning Over-parameterized ResNet: A Full Characterization. arXiv preprint arXiv:1903.07120, 2019.
  \bibitem{yuanzhi18}
  Yuanzhi L. and Yingyu L. Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data. InAdvances in Neural Information Processing Systems, pages 8157–8166, 2018.
  \bibitem{difan18}
  Difan Z., Yuan C., Dongruo Z., and Quanquan G. Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks. arXiv preprint arXiv:1811.08888, 2018.
  \bibitem{jonathan18}
  Jonathan F., and Michael C. The Lottery Ticket Hypotheis: Finding Sparse, Trainable Neural Networks. In7th International Conference on Learning Representations, ICLR, 2019.
  \bibitem{li17}
  Li, Y., Ma, T., \& Zhang, H. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. arXiv preprint arXiv:1712.09203, 2017.
  \bibitem{du18}
  Du, S. S., Hu, W., \& Lee, J. D. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems (pp. 384-395).
  % 1.3
  \bibitem{chen98}
  S.S. Chen, D.L. Donoho, M.A. Saunders. Atomic Decomposition by Basis Pursuit. SIAM Journal on Scientific Computing 20(1), p.33-61, 1998.
  \bibitem{tibshirani96}
  R. Tibshirani. Regression Shrinkage and Selection via the lasso. Journal of the Royal Statistical Society: Series B 58(1), p.267–88, 1996.
  % 1.6
  \bibitem{wolpert97}
  Wolpert, D. H. , and W. G. Macready . No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation 1.1(1997):67-82.
  %1.7
  \bibitem{wolpert06}
  Wolpert, David H , and W. G. Macready . Coevolutionary Free Lunches. IEEE Transactions on Evolutionary Computation 9.6(2006):721-735.
  % 1.8
  \bibitem{jure20}
  Jure Leskovec, Anand Rajaraman, Jeff Ullman. Mining of Massive Datasets. Cambridge Press, 2020.
  % 1.10
  \bibitem{srinivas10}
  Srinivas, N., et. al. Gaussian process optimization in the bandit setting: No regret and experimental design. ICML 2010.
  \bibitem{jones98}
  Jones, D., et. al., Efficient global optimization of expensive black-box functions. J. Global Optimization, 1998.
\end{thebibliography}